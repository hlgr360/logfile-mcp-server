````markdown
# Implementation Clarifications

## Technical Architecture Decisions

### 1. Development Approach
**Decision**: Create a completely new application from scratch.

**Rationale**: There are no existing components to build upon. Start with a fresh project structure as outlined in the specification.

### 2. Database Schema Implementation
**Decision**: Implement as separate SQLAlchemy models (`NginxLog` and `NexusLog`).

**Rationale**: 
- The schemas have meaningful differences (dual response sizes in Nexus, thread info, etc.) 
- Separate models align with the modular architecture for easy extension to new log formats
- Each format warrants its own optimized structure

### 3. MCP Server Integration Architecture
**Decision**: Single process with optional MCP server enabled/disabled via configuration.

**Implementation**:
- Web server: default port 8000
- MCP server: default port 8001 (only when `--enable-mcp-server` is set)
- Both servers run concurrently in the same process when MCP is enabled

## File Processing Decisions

### 1. Archive Processing Priority
**Decision**: Process both unarchived files AND archive files.

**Rationale**:
- Provides comprehensive analysis
- Different files may contain different time periods
- Database performance can handle duplicates
- Users may want to correlate same events from different sources
- Don't implement duplicate detection - process everything

### 2. Nested Archive Depth Configuration
**Decision**: Make maximum depth configurable with default of 3.

**Implementation**:
```bash
# CLI option
--max-archive-depth 3

# Environment variable
MAX_ARCHIVE_DEPTH=3
```

**Default**: 3 levels of nesting

## Configuration Decisions

### 1. File Pattern Matching Implementation
**Decision**: Use shell glob patterns only (`*`, `?`).

**Rationale**:
- All examples provided (`request.log*`, `nexus_logs_*.tar`) are glob patterns
- Simpler and more intuitive for users than regex
- Sufficient for the use cases described

**Not Implemented**: Regular expression support

### 2. Database Recreation Behavior
**Decision**: Always drop/recreate database on application start.

**Rationale**: 
- This is explicitly specified: "Fresh database creation on each application start"
- Intentional design: "database should be created anew on each app restart"
- No configurable behavior needed

## Testing and Development Workflow

### 1. Test Data Creation
**Decision**: Create the complete test data structure as specified in the documentation.

**Implementation**: Generate sample data programmatically in test setup to ensure consistency and reproducibility.

**Required Structure**:
```
tests/sample_data/
├── nginx/
│   ├── access.log
│   ├── access.log.1.gz
│   ├── monthly_backup.tar.gz
│   ├── daily_logs.zip
│   └── nested_backup.zip
├── nexus/
│   ├── request.log
│   ├── request.log.1.gz
│   ├── nexus_logs_20250612_112311.tar
│   └── weekly_archive.tar
└── malformed/
    ├── bad_nginx_access.log.gz
    ├── bad_nexus_logs.tar
    └── deeply_nested.zip
```

### 2. Development Workflow and Branching Strategy
**Decision**: Break implementation into logical feature branches for better review and testing.

## Implementation Priority and Phases

### Phase 1: Foundation (`feature/project-setup`)
**Scope**:
- Project structure and dependencies (`pyproject.toml`)
- Configuration management (CLI args, .env support)
- Basic database models and connection setup
- Configuration validation

**Deliverable**: Runnable app skeleton with config validation

**Success Criteria**:
- App starts with proper CLI argument parsing
- Database connection established
- Configuration loaded from .env file
- Basic error handling for invalid config

### Phase 2: Core Processing (`feature/log-processing`)
**Scope**:
- File discovery and pattern matching
- Archive extraction (including nested with depth limits)
- Base processor abstract class
- Nginx and Nexus processor implementations
- Chunked file processing and batch database inserts

**Deliverable**: Command-line app that processes logs to database

**Success Criteria**:
- Processes all supported archive formats
- Handles nested archives with depth protection
- Parses nginx and nexus log formats correctly
- Stores parsed data in separate database tables
- Proper error logging to stdout

### Phase 3: Web Interface (`feature/web-interface`)
**Scope**:
- FastAPI application setup
- HTML templates with stable IDs
- API endpoints for table previews
- SQL query interface with security restrictions
- Results display and error handling

**Deliverable**: Working web application

**Success Criteria**:
- Web interface displays first 10 rows from both tables
- SQL query execution with SELECT-only restriction
- Results formatted and displayed properly
- All HTML elements have stable IDs for testing

### Phase 4: MCP Integration (`feature/mcp-server`)
**Scope**:
- MCP server implementation
- Database schema inspection tools
- Three MCP tools: list_database_schema, execute_sql_query, get_table_sample
- Concurrent operation with web server

**Deliverable**: LLM integration capability

**Success Criteria**:
- MCP server runs on separate port when enabled
- All three MCP tools function correctly
- Same security restrictions as web interface
- Proper JSON formatting for LLM consumption

### Phase 5: Testing & Polish (`feature/comprehensive-tests`)
**Scope**:
- Unit and integration tests
- Playwright E2E tests with test data setup
- Documentation updates
- Performance optimizations
- Final error handling improvements

**Deliverable**: Production-ready application

**Success Criteria**:
- Full test coverage across all components
- All Playwright tests pass with stable selectors
- README and documentation complete
- Performance meets specification requirements

## Key Implementation Guidelines

### Error Handling Format
**Required Format**: `PARSE_ERROR: {file_path}:{line_number} - {error_message}`

**Example**: 
```
PARSE_ERROR: /logs/nginx/access.log.gz:1247 - Invalid timestamp format: [invalid-date]
```

### File Source Tracking
**For Nested Archives**: Use arrow notation to show extraction chain

**Format**: `outer.zip->inner.tar.gz->file.log`

**Example**: `backup.zip->daily.tar.gz->access.log:line_123`

### Memory Management
**Requirements**:
- Use generators for file processing
- Process in batches of 1000 entries
- Batch database inserts with transactions
- Proper resource cleanup with context managers

### Security Implementation
**SQL Query Restrictions**:
- Only SELECT statements allowed
- Maximum 1000 rows returned
- Query timeout enforcement
- Path validation for file access

### Configuration Defaults
```python
# Default configuration values
DEFAULT_VALUES = {
    "chunk_size": 8192,
    "line_buffer_size": 1000,
    "max_archive_depth": 3,
    "web_port": 8000,
    "mcp_port": 8001,
    "enable_mcp_server": False,
    "db_name": "log_analysis.db"
}
```

## Starting Implementation

**First Steps**:
1. Create `feature/project-setup` branch
2. Set up project structure per specification
3. Implement configuration management
4. Create basic database models
5. Add configuration validation

**First PR Should Include**:
- Complete project directory structure
- Working CLI argument parsing
- Environment file support
- Database connection setup
- Basic unit tests for configuration

**Definition of Done for Phase 1**:
- App starts without errors
- Validates all configuration parameters
- Connects to SQLite database
- Creates database schema
- Passes basic configuration tests

This clarification document provides definitive answers to all implementation questions and should be referenced throughout the development process.

````
